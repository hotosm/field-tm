# Copyright (c) Humanitarian OpenStreetMap Team
# This file is part of Field-TM.
#
#     Field-TM is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     Field-TM is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with Field-TM.  If not, see <https:#www.gnu.org/licenses/>.
#

name: fmtm

volumes:
  fmtm_data:
  fmtm_db_data:
  fmtm_logs:
  fmtm_images:
  fmtm_tiles:
  central_db_data:
  central_frontend:
  qfield_db_data:
  qfield_static:
  qfield_projects:

networks:
  fmtm-net:
    name: fmtm-local
    ipam:
      driver: default
      config:
        - subnet: 10.20.30.0/24

services:
  proxy:
    image: "bunkerity/bunkerweb-all-in-one:1.6.2"
    depends_on:
      # Frontends must be built and available first
      ui:
        condition: service_started
      ui-mapper:
        condition: service_started
      central-ui:
        condition: service_completed_successfully
        required: false
    volumes:
      - central_frontend:/var/www/html/odk.fmtm.localhost:ro
      - qfield_static:/var/www/html/qfield.fmtm.localhost/staticfiles:ro
    ports:
      - ${FMTM_DEV_PORT:-7050}:8080
    environment:
      # General
      BUNKERWEB_INSTANCES: proxy:5000
      LOG_LEVEL: notice
      UI_WIZARD: no
      USE_BUNKERNET: no
      DISABLE_DEFAULT_SERVER: yes
      USE_REDIS: no
      API_WHITELIST_IP: 127.0.0.0/8 10.20.30.0/24
      # Avoid running ModSec rules on internal service calls
      WHITELIST_IP: 10.20.30.0/24
      WHITELIST_URI: http://fmtm.localhost:7050
      MULTISITE: yes
      USE_REVERSE_PROXY: yes
      # Required for electric headers electric-offset, electric-handle, electric-schema, electric-cursor
      KEEP_UPSTREAM_HEADERS: yes
      REVERSE_PROXY_INTERCEPT_ERRORS: no
      ALLOWED_METHODS: OPTIONS|HEAD|GET|POST|PATCH|PUT|DELETE
      USE_REAL_IP: yes
      SERVE_FILES: yes
      USE_BACKUP: no
      USE_METRICS: no
      # USE_ANTIBOT: yes
      USE_LIMIT_CONN: no
      USE_BAD_BEHAVIOR: no
      USE_LIMIT_REQ: no
      USE_MODSECURITY: no
      USE_GZIP: yes
      # On client, brotli is preferred over gzip if both are enabled
      USE_BROTLI: yes
      # Reverse proxy configs
      SERVER_NAME: fmtm.localhost mapper.fmtm.localhost api.fmtm.localhost s3.fmtm.localhost sync.fmtm.localhost odk.fmtm.localhost qfield.fmtm.localhost
      fmtm.localhost_REVERSE_PROXY_HOST: http://ui:7051
      fmtm.localhost_MAX_CLIENT_SIZE: 1G
      # We allow accelerometer, camera, fullscreen, geolocation, magnetometer, gyroscope
      fmtm.localhost_PERMISSIONS_POLICY: accelerometer=(self), ambient-light-sensor=(), attribution-reporting=(), autoplay=(), battery=(), bluetooth=(), browsing-topics=(), camera=(self), compute-pressure=(), display-capture=(), encrypted-media=(), execution-while-not-rendered=(), execution-while-out-of-viewport=(), fullscreen=(self), gamepad=(), geolocation=(self), gyroscope=(self), hid=(), identity-credentials-get=(), idle-detection=(), local-fonts=(), magnetometer=(self), microphone=(), midi=(), otp-credentials=(), payment=(), picture-in-picture=(), publickey-credentials-create=(), publickey-credentials-get=(), screen-wake-lock=(), serial=(), speaker-selection=(), storage-access=(), usb=(), web-share=(), window-management=(), xr-spatial-tracking=(), interest-cohort=()
      # Required for vite websockets / live-reload
      fmtm.localhost_REVERSE_PROXY_WS: yes
      mapper.fmtm.localhost_REVERSE_PROXY_HOST: http://ui-mapper:7057
      mapper.fmtm.localhost_MAX_CLIENT_SIZE: 1G
      # Required for vite websockets / live-reload
      mapper.fmtm.localhost_REVERSE_PROXY_WS: yes
      api.fmtm.localhost_REVERSE_PROXY_HOST: http://api:8000
      api.fmtm.localhost_MAX_CLIENT_SIZE: 1G
      # Increase timeout slightly for long project creation requests
      api.fmtm.localhost_REVERSE_PROXY_READ_TIMEOUT: 90s
      s3.fmtm.localhost_REVERSE_PROXY_HOST: http://s3:9000
      s3.fmtm.localhost_MAX_CLIENT_SIZE: 10G
      sync.fmtm.localhost_REVERSE_PROXY_HOST: http://electric:3000
      sync.fmtm.localhost_MAX_CLIENT_SIZE: 1G
      # Expose-Headers are RESPONSE headers the browser is allowed to access
      sync.fmtm.localhost_CORS_EXPOSE_HEADERS: electric-offset,electric-handle,electric-schema,electric-cursor,electric-up-to-date
      odk.fmtm.localhost_REVERSE_PROXY_HOST: http://central:8383
      # Required to allow loading frontend
      odk.fmtm.localhost_REVERSE_PROXY_URL: ~ ^/v\d
      # buffer requests, but not responses, so streaming out works
      odk.fmtm.localhost_REVERSE_PROXY_BUFFERING: no
      odk.fmtm.localhost_MAX_CLIENT_SIZE: 1G
      # Override X-Forwarded-Proto during local dev to allow auth over http
      odk.fmtm.localhost_REVERSE_PROXY_HEADERS: "X-Forwarded-Proto http"
      qfield.fmtm.localhost_REVERSE_PROXY_HOST: http://qfield-app:8000
      # Required to allow loading staticfiles (else route to reverse proxy)
      qfield.fmtm.localhost_REVERSE_PROXY_URL: ~ ^/(?!staticfiles)
      # Sometimes we have large base imagery mbtile layers...
      qfield.fmtm.localhost_MAX_CLIENT_SIZE: 2G
      # Override X-Forwarded-Proto during local dev to allow auth over http
      # https://docs.djangoproject.com/en/4.2/ref/settings/#secure-proxy-ssl-header
      # NOTE we also override the origin to remove the port & prefer CSRF issues
      qfield.fmtm.localhost_REVERSE_PROXY_HEADERS: "X-Forwarded-Proto http; Origin http://qfield.fmtm.localhost"
    networks:
      fmtm-net:
        ipv4_address: 10.20.30.50
    restart: "unless-stopped"

  api:
    image: "ghcr.io/hotosm/field-tm/backend:${TAG_OVERRIDE:-debug}"
    build:
      context: src
      dockerfile: backend/Dockerfile
      target: "${TARGET_OVERRIDE:-debug}"
      args:
        APP_VERSION: "${TAG_OVERRIDE:-debug}"
    # Uncomment these to debug with a terminal debugger like pdb
    # Then `docker attach fmtm_api` to debug
    # stdin_open: true
    # tty: true
    volumes:
      - fmtm_logs:/opt/logs
      - qfield_projects:/opt/qfield
      - ./src/backend/pyproject.toml:/opt/pyproject.toml:ro
      - ./src/backend/app:/opt/app:ro
      - ./src/backend/tests:/opt/tests:ro
      - ./src/backend/scheduler:/opt/scheduler:ro
      - ./src/backend/stats:/opt/stats:ro
      # Workspace packages config
      - ./src/backend/packages/osm-fieldwork/osm_fieldwork:/opt/python/lib/python3.13/site-packages/osm_fieldwork:ro
      - ./src/backend/packages/osm-fieldwork/tests:/opt/package_tests/test_osm_fieldwork:ro
      - ./src/backend/packages/area-splitter/area_splitter:/opt/python/lib/python3.13/site-packages/area_splitter:ro
      - ./src/backend/packages/area-splitter/tests:/opt/package_tests/test_area_splitter:ro
    environment:
      DEBUG: ${DEBUG:-True}
    depends_on:
      proxy:
        condition: service_healthy
      fmtm-db:
        condition: service_healthy
      central:
        condition: service_healthy
        required: false
      qfield-init:
        condition: service_completed_successfully
      qfield-qgis:
        condition: service_healthy
      migrations:
        condition: service_completed_successfully
      s3:
        condition: service_healthy
    env_file:
      - .env
    ports:
      - "7052-7055:8000"
      - "5678-5679:5678"
    networks:
      - fmtm-net
    restart: "unless-stopped"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/__lbheartbeat__"]
      start_period: 60s
      interval: 10s
      timeout: 5s
      retries: 10
    deploy:
      replicas: ${API_REPLICAS:-1}
      resources:
        limits:
          cpus: "0.9"
          memory: 1500M
        reservations:
          cpus: "0.1"
          memory: 100M

  ui:
    image: "ghcr.io/hotosm/field-tm/frontend:debug"
    build:
      context: src
      additional_contexts:
        - code=src/frontend
      dockerfile: Dockerfile.ui.debug
      target: build
    volumes:
      - ./src/frontend/e2e:/app/e2e
      - ./src/frontend/public:/app/public
      - ./src/frontend/src:/app/src
      - ./src/frontend/tests:/app/tests
      - ./src/frontend/index.html:/app/index.html
      - ./src/frontend/package.json:/app/package.json
      - ./src/frontend/playwright.config.ts:/app/playwright.config.ts
      - ./src/frontend/postcss.config.js:/app/postcss.config.js
      - ./src/frontend/tailwind.config.js:/app/tailwind.config.js
      - ./src/frontend/vite.config.ts:/app/vite.config.ts
    environment:
      - VITE_API_URL=http://api.${FMTM_DOMAIN}:${FMTM_DEV_PORT:-7050}
    ports:
      - "7051:7051"
    networks:
      - fmtm-net
    restart: "unless-stopped"

  ui-mapper:
    image: "ghcr.io/hotosm/field-tm/frontend:mapper"
    build:
      context: src
      additional_contexts:
        - code=src/mapper
      dockerfile: Dockerfile.ui.debug
    volumes:
      - ./src/mapper/messages:/app/messages
      - ./src/mapper/project.inlang:/app/project.inlang
      - ./src/mapper/src:/app/src
      - ./src/mapper/static:/app/static
      - ./src/mapper/tests:/app/tests
      - ./src/mapper/package.json:/app/package.json
      - ./src/mapper/playwright.config.ts:/app/playwright.config.ts
      - ./src/mapper/svelte.config.js:/app/svelte.config.js
      - ./src/mapper/uno.config.ts:/app/uno.config.ts
      - ./src/mapper/vite.config.ts:/app/vite.config.ts
      # - ../ui:/app/node_modules/@hotosm/ui:ro
    environment:
      - VITE_API_URL=http://api.${FMTM_DOMAIN}:${FMTM_DEV_PORT:-7050}
      - VITE_SYNC_URL=http://sync.${FMTM_DOMAIN}:${FMTM_DEV_PORT:-7050}
      - VITE_S3_URL=http://s3.${FMTM_DOMAIN}:${FMTM_DEV_PORT:-7050}
    ports:
      - "7057:7057"
    networks:
      - fmtm-net
    restart: "unless-stopped"
    command: pnpm run dev

  central:
    profiles: ["", "central"]
    image: "ghcr.io/hotosm/field-tm/odkcentral:v2025.2.1"
    build:
      context: odkcentral/api
      args:
        ODK_CENTRAL_TAG: v2025.2.0
    depends_on:
      proxy:
        condition: service_healthy
      central-db:
        condition: service_healthy
      s3:
        condition: service_healthy
      pyxform:
        condition: service_started
    environment:
      - DOMAIN=${FMTM_ODK_DOMAIN:-odk.fmtm.localhost}:${FMTM_DEV_PORT:-7050}
      - SSL_TYPE=upstream
      - SYSADMIN_EMAIL=${ODK_CENTRAL_USER}
      - SYSADMIN_PASSWD=${ODK_CENTRAL_PASSWD}
      - HTTPS_PORT=${HTTPS_PORT:-443}
      - DB_HOST=${CENTRAL_DB_HOST:-central-db}
      - DB_USER=${CENTRAL_DB_USER:-odk}
      - DB_PASSWORD=${CENTRAL_DB_PASSWORD:-odk}
      - DB_NAME=${CENTRAL_DB_NAME:-odk}
      - DB_SSL=${DB_SSL:-null}
      - EMAIL_FROM=${ODK_CENTRAL_USER}
      - EMAIL_HOST=${EMAIL_HOST:-mail}
      - EMAIL_PORT=${EMAIL_PORT:-25}
      - EMAIL_SECURE=${EMAIL_SECURE:-false}
      - EMAIL_IGNORE_TLS=${EMAIL_IGNORE_TLS:-true}
      - EMAIL_USER=${EMAIL_USER:-''}
      - EMAIL_PASSWORD=${EMAIL_PASSWORD:-''}
      - OIDC_ENABLED=${OIDC_ENABLED:-false}
      - OIDC_ISSUER_URL=${OIDC_ISSUER_URL:-https://getodk.org}
      - OIDC_CLIENT_ID=${OIDC_CLIENT_ID:-xxx}
      - OIDC_CLIENT_SECRET=${OIDC_CLIENT_SECRET:-xxx}
      - SENTRY_ORG_SUBDOMAIN=${SENTRY_ORG_SUBDOMAIN:-o130137}
      - SENTRY_KEY=${SENTRY_KEY:-3cf75f54983e473da6bd07daddf0d2ee}
      - SENTRY_PROJECT=${SENTRY_PROJECT:-1298632}
      - SENTRY_TRACE_RATE=${SENTRY_TRACE_RATE:-100000}
      # Note S3_ENDPOINT is hardcoded here for when we use tunnel config
      - S3_SERVER="http://s3:9000"
      - S3_BUCKET_NAME=${S3_ODK_BUCKET_NAME:-"fmtm-odk-media"}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
    # ports:
    #   - "8383:8383"
    networks:
      - fmtm-net
    restart: "unless-stopped"
    healthcheck:
      test: nc -z localhost 8383 || exit 1
      start_period: 15s
      interval: 10s
      timeout: 5s
      retries: 10

  pyxform:
    image: "ghcr.io/getodk/pyxform-http:v3.0.0"
    networks:
      - fmtm-net
    restart: "unless-stopped"

  central-ui:
    # This service simply builds the frontend to a volume
    # accessible to the proxy, then shuts down
    profiles: ["", "central"]
    image: "ghcr.io/hotosm/field-tm/odkcentral-ui:v2025.2.1"
    build:
      context: odkcentral/ui
      args:
        ODK_CENTRAL_TAG: v2025.2.0
    volumes:
      - central_frontend:/frontend
    network_mode: none
    restart: "on-failure:2"

  central-webhook:
    profiles: ["", "central"]
    image: "ghcr.io/hotosm/central-webhook:0.3.0"
    depends_on:
      central:
        condition: service_healthy
    environment:
      CENTRAL_WEBHOOK_DB_URI: postgresql://${CENTRAL_DB_USER:-odk}:${CENTRAL_DB_USER:-odk}@central-db:5432/${CENTRAL_DB_NAME:-odk}?sslmode=disable
      CENTRAL_WEBHOOK_UPDATE_ENTITY_URL: http://api:8000/integrations/webhooks/entity-status
      # CENTRAL_WEBHOOK_REVIEW_SUBMISSION_URL: https://your.domain.com/some/webhook
      CENTRAL_WEBHOOK_API_KEY: qnyE7ev7OWsfMAaX2fm-PuWYnkAUJw2xlyp72FKCH3Q
      # CENTRAL_WEBHOOK_LOG_LEVEL: DEBUG
    networks:
      - fmtm-net
    restart: "unless-stopped"

  s3:
    image: "docker.io/minio/minio:RELEASE.2025-01-20T14-49-07Z"
    depends_on:
      proxy:
        condition: service_started
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY:-fmtm}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY:-somelongpassword}
      MINIO_VOLUMES: "/mnt/data"
      MINIO_BROWSER: ${MINIO_BROWSER:-off}
    volumes:
      - fmtm_data:/mnt/data
    # ports:
    # - 9000:9000
    # - 9090:9090
    networks:
      - fmtm-net
    command: minio server
    restart: "unless-stopped"
    healthcheck:
      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1
      interval: 5s
      retries: 3
      start_period: 5s
      timeout: 5s

  fmtm-db:
    # Temp workaround until https://github.com/postgis/docker-postgis/issues/216
    image: "ghcr.io/hotosm/postgis:16-3.5-alpine"
    volumes:
      - fmtm_db_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=${FMTM_DB_USER:-fmtm}
      - POSTGRES_PASSWORD=${FMTM_DB_PASSWORD:-fmtm}
      - POSTGRES_DB=${FMTM_DB_NAME:-fmtm}
    ports:
      - "5438:5432"
    networks:
      - fmtm-net
    restart: "unless-stopped"
    # We don't need to optimise connections like the prod deploy here.
    # Conversely, we increase connections for test runners.
    command: -c 'max_connections=300' -c 'wal_level=logical'
    healthcheck:
      test: pg_isready -U ${FMTM_DB_USER:-fmtm} -d ${FMTM_DB_NAME:-fmtm}
      start_period: 5s
      interval: 10s
      timeout: 5s
      retries: 3

  electric:
    image: "electricsql/electric:1.0.22"
    depends_on:
      proxy:
        condition: service_started
      fmtm-db:
        condition: service_healthy
      migrations:
        condition: service_completed_successfully
    environment:
      DATABASE_URL: postgresql://${FMTM_DB_USER:-fmtm}:${FMTM_DB_PASSWORD:-fmtm}@${FMTM_DB_HOST:-fmtm-db}/${FMTM_DB_NAME:-fmtm}?sslmode=disable
      # For development we do not need to include auth token security
      ELECTRIC_INSECURE: true
      # OTEL_EXPORT: otlp
      # OTLP_ENDPOINT: https://...
      # ELECTRIC_WRITE_TO_PG_MODE: direct_writes
    networks:
      - fmtm-net
    restart: "unless-stopped"

  central-db:
    profiles: ["", "central"]
    image: "ghcr.io/hotosm/postgis:14-3.5-alpine"
    volumes:
      - central_db_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=${CENTRAL_DB_USER:-odk}
      - POSTGRES_PASSWORD=${CENTRAL_DB_PASSWORD:-odk}
      - POSTGRES_DB=${CENTRAL_DB_NAME:-odk}
    ports:
      - "5434:5432"
    networks:
      - fmtm-net
    restart: "unless-stopped"
    # Optimise to a low number of connections, see fmtm-db service
    command: -c 'max_connections=64'
    healthcheck:
      test: pg_isready -U ${CENTRAL_DB_USER:-odk} -d ${CENTRAL_DB_NAME:-odk}
      start_period: 5s
      interval: 10s
      timeout: 5s
      retries: 3

  migrations:
    image: "ghcr.io/hotosm/field-tm/backend:${TAG_OVERRIDE:-debug}"
    depends_on:
      fmtm-db:
        condition: service_healthy
      s3:
        condition: service_healthy
    env_file:
      - .env
    # Hardcode some vars for dev, as not necessarily present in the .env file
    environment:
      # Note S3_ENDPOINT is hardcoded here for when we use tunnel config
      - S3_ENDPOINT=http://s3:9000
      - S3_BUCKET_NAME=${S3_BUCKET_NAME:-"fmtm-data"}
      - S3_BACKUP_BUCKET_NAME=${S3_BACKUP_BUCKET_NAME:-"fmtm-db-backups"}
      - S3_QFIELD_BUCKET_NAME=${S3_QFIELD_BUCKET_NAME:-"qfield-cloud"}
    networks:
      - fmtm-net
    entrypoint: ["/migrate-entrypoint.sh"]
    restart: "on-failure:2"
    healthcheck:
      test: ["NONE"] # Set the health check test to NONE to disable it

  scheduler:
    image: "ghcr.io/hotosm/field-tm/backend:${TAG_OVERRIDE:-debug}"
    depends_on:
      fmtm-db:
        condition: service_healthy
    env_file:
      - .env
    environment:
      DEBUG: false
    networks:
      - fmtm-net
    entrypoint: ["/bin/sh", "-c"]
    # The approach below allows us to easily switch to Kubernetes CronJob if needed
    command: |
      "
        # Task unlocking every 3hrs
        echo '* */3 * * * /opt/scheduler/unlock_tasks.py' > ./crontab

        # Check inactive users every Sunday 00:00
        echo '0 0 * * 0 /opt/scheduler/inactive_users.py' >> ./crontab

        # Run project stats script every 10 mins
        echo '*/10 * * * * /opt/scheduler/project_stats.py' >> ./crontab

        # Upload and update project submissions to s3 every hours
        echo '0 * * * * /opt/scheduler/upload_submissions_to_s3.py' >> ./crontab

        # Refresh user submission counts once a day
        echo '0 0 * * * /opt/scheduler/user_submission_stats.py' >> ./crontab

        exec /usr/local/bin/supercronic ./crontab
      "
    restart: "unless-stopped"
    # Check the 'supercronic' service is still running
    healthcheck:
      test: ["CMD", "pgrep", "supercronic"]
      interval: 5m
      timeout: 10s
      retries: 3
      start_period: 10s

  qfield-db:
    # Temp workaround until https://github.com/postgis/docker-postgis/issues/216
    image: "ghcr.io/hotosm/postgis:16-3.5-alpine"
    volumes:
      - qfield_db_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=${FMTM_DB_USER:-fmtm}
      - POSTGRES_PASSWORD=${FMTM_DB_PASSWORD:-fmtm}
      - POSTGRES_DB=${FMTM_DB_NAME:-fmtm}
    ports:
      - "5435:5432"
    networks:
      - fmtm-net
    restart: "unless-stopped"
    # Optimise to a low number of connections, see fmtm-db service
    command: -c 'max_connections=64'
    healthcheck:
      test: pg_isready -U ${FMTM_DB_USER:-fmtm} -d ${FMTM_DB_NAME:-fmtm}
      start_period: 5s
      interval: 10s
      timeout: 5s
      retries: 3

  qfield-app:
    image: ghcr.io/opengisch/qfieldcloud-app:25.24
    volumes:
      - qfield_static:/usr/src/app/staticfiles
    networks:
      - fmtm-net
    restart: "unless-stopped"
    environment:
      DJANGO_SETTINGS_MODULE: qfieldcloud.settings
      DEBUG: 1
      ENVIRONMENT: development
      QFIELDCLOUD_HOST: qfield.fmtm.localhost
      DJANGO_ALLOWED_HOSTS: "qfield.fmtm.localhost localhost 127.0.0.1 0.0.0.0 qfield-app proxy"
      DJANGO_SUPERUSER_USERNAME: admin
      DJANGO_SUPERUSER_EMAIL: sysadmin@hotosm.org
      DJANGO_SUPERUSER_PASSWORD: ${QFIELDCLOUD_ADMIN_PASSWORD}
      SECRET_KEY: YKuuTCb256dMmcg4DxpxrCafrcVvk8s3ZJrtDr9JaVAkeLiULGiemyzUTQESrzPQhiYUfn
      SALT_KEY: 0123456789abcdefghijklmnopqrstuvwxyz
      ACCOUNT_EMAIL_VERIFICATION: none
      # TODO fix injecting S3 vars into STORAGES
      # STORAGE_ACCESS_KEY_ID: ${S3_ACCESS_KEY:-fmtm}
      # STORAGE_SECRET_ACCESS_KEY: ${S3_SECRET_KEY:-somelongpassword}
      # STORAGE_BUCKET_NAME: ${S3_QFIELD_BUCKET_NAME:-"qfield-cloud"}
      # STORAGE_ENDPOINT_URL: http://s3:9000
      STORAGES: '{
        "default": {
        "BACKEND": "qfieldcloud.filestorage.backend.QfcS3Boto3Storage",
        "OPTIONS": {
        "access_key": "fmtm",
        "secret_key": "somelongpassword",
        "bucket_name": "qfield-cloud",
        "region_name": "",
        "endpoint_url": "http://s3:9000"
        },
        "QFC_IS_LEGACY": false
        }
        }'
      POSTGRES_USER: ${FMTM_DB_USER:-fmtm}
      POSTGRES_PASSWORD: ${FMTM_DB_PASSWORD:-fmtm}
      POSTGRES_DB: ${FMTM_DB_NAME:-fmtm}
      POSTGRES_HOST: qfield-db
      POSTGRES_PORT: 5432
      GEODB_HOST: qfield-db
      GEODB_PORT: 5432
      GEODB_USER: ${FMTM_DB_USER:-fmtm}
      GEODB_PASSWORD: ${FMTM_DB_PASSWORD:-fmtm}
      GEODB_DB: ${FMTM_DB_NAME:-fmtm}
      QFIELDCLOUD_QGIS_IMAGE_NAME: ghcr.io/opengisch/qfieldcloud-qgis:25.24
      # FIXME add actual worker
      QFIELDCLOUD_WORKER_QFIELDCLOUD_URL: http://localhost:8000
      QFIELDCLOUD_AUTH_TOKEN_EXPIRATION_HOURS: 720
      QFIELDCLOUD_USE_I18N: 0
      QFIELDCLOUD_DEFAULT_LANGUAGE: "en"
      QFIELDCLOUD_DEFAULT_TIME_ZONE: "Europe/London"
      # QFIELDCLOUD_DEFAULT_NETWORK: ${QFIELDCLOUD_DEFAULT_NETWORK:-${COMPOSE_PROJECT_NAME}_default}
      # QFIELDCLOUD_SUBSCRIPTION_MODEL: ${QFIELDCLOUD_SUBSCRIPTION_MODEL}
      # QFIELDCLOUD_ACCOUNT_ADAPTER: ${QFIELDCLOUD_ACCOUNT_ADAPTER}
      # QFIELDCLOUD_TRANSFORMATION_GRIDS_VOLUME_NAME: ${COMPOSE_PROJECT_NAME}_transformation_grids
      # EMAIL_HOST: ${EMAIL_HOST}
      # EMAIL_USE_TLS: ${EMAIL_USE_TLS}
      # EMAIL_USE_SSL: ${EMAIL_USE_SSL}
      # EMAIL_PORT: ${EMAIL_PORT}
      # EMAIL_HOST_USER: ${EMAIL_HOST_USER}
      # EMAIL_HOST_PASSWORD: ${EMAIL_HOST_PASSWORD}
      # DEFAULT_FROM_EMAIL: ${DEFAULT_FROM_EMAIL}
      # For qfield-init:
      DEFAULT_ORG_NAME: ${DEFAULT_ORG_NAME}
      QFIELDCLOUD_USER: ${QFIELDCLOUD_USER}
      QFIELDCLOUD_PASSWORD: ${QFIELDCLOUD_PASSWORD}
      QFIELDCLOUD_ORG_NAME: ${QFIELDCLOUD_ORG_NAME}
    command:
      - sh
      - -c
      - |
        python manage.py migrate &&
        python manage.py collectstatic --noinput &&
        python manage.py createsuperuser --no-input || true &&
        python manage.py compilemessages &&
        exec gunicorn qfieldcloud.wsgi:application \
          --bind 0.0.0.0:8000 \
          --timeout 300 \
          --max-requests 300 \
          --workers 1 \
          --threads 4
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/status"]
      start_period: 30s
      interval: 10s
      timeout: 5s
      retries: 10

  qfield-init:
    extends: qfield-app
    depends_on:
      qfield-app:
        condition: service_healthy
    restart: "on-failure:2"
    command:
      - sh
      - -c
      - |
        echo "Waiting for app to be ready..."
        until curl -sf http://qfield-app:8000/api/v1/status > /dev/null; do
          sleep 3
        done

        echo "Initializing organizations and users..."
        python - <<'PYCODE'
        import os, sys, django
        django.setup()

        from qfieldcloud.core.models import Organization, Person, User

        username = os.getenv("QFIELDCLOUD_USER")
        email = "svcfmtm@hotosm.org"
        password = os.getenv("QFIELDCLOUD_PASSWORD")
        org_name = os.getenv("DEFAULT_ORG_NAME")

        if not all([username, password, org_name]):
            print("QFIELDCLOUD_USER or QFIELDCLOUD_PASSWORD or DEFAULT_ORG_NAME not set")
            sys.exit(1)

        # Create or get the person
        user, _ = Person.objects.get_or_create(
            username=username,
            defaults={
                "email": email,
                "type": User.Type.PERSON,
                "has_accepted_tos": True,
            },
        )

        # Create or get the organization
        org, _ = Organization.objects.get_or_create(
            username=org_name,
            defaults={
                "organization_owner": user,
                "created_by": user,
                "type": User.Type.ORGANIZATION,
                "is_initially_trial": False,
            },
        )

        print(f"Initialized organization: {org.username}, owner: {user.username}")
        PYCODE

  qfield-qgis:
    image: "ghcr.io/hotosm/field-tm/qgis:25.24"
    build: qfield
    volumes:
      - qfield_projects:/opt/qfield
      - ./qfield/project_gen_svc.py:/usr/src/app/project_gen_svc.py:ro
    environment:
      LOG_LEVEL: DEBUG
    networks:
      - fmtm-net
    restart: "unless-stopped"
    healthcheck:
      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/8080' || exit 1
      start_period: 10s
      interval: 5s
      retries: 5
      timeout: 5s
